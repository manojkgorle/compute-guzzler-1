{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 From Scratch â€” Training\n",
    "\n",
    "Train GPT-2 Small (124M parameters) on WikiText-2 from scratch.\n",
    "\n",
    "**Architecture**: 12 layers, 768 hidden dim, 12 attention heads, 512 context length  \n",
    "**Dataset**: WikiText-2 (~2.4M tokens from Wikipedia)  \n",
    "**Hardware**: Works on CUDA (T4/Colab), MPS (Apple Silicon), or CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q torch tiktoken datasets numpy"
  },
  {
   "cell_type": "code",
   "source": "!git clone https://github.com/manojkgorle/compute-guzzler-1.git gpt2-vc 2>/dev/null || echo \"Already cloned\"\n%cd gpt2-vc",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from config import GPT2Config, TrainConfig, get_device\n",
    "from model import GPT2\n",
    "from data import create_dataloaders\n",
    "from train import train\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config()\n",
    "train_config = TrainConfig(\n",
    "    max_epochs=30,\n",
    "    batch_size=8,\n",
    "    learning_rate=3e-4,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(f\"Context length: {config.context_length}\")\n",
    "print(f\"Batch size: {train_config.batch_size} \"\n",
    "      f\"(effective: {train_config.batch_size * train_config.gradient_accumulation_steps})\")\n",
    "print(f\"Epochs: {train_config.max_epochs}\")\n",
    "print(f\"Learning rate: {train_config.learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = create_dataloaders(config, train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train\n",
    "\n",
    "On CUDA this automatically enables:\n",
    "- `torch.compile` (kernel fusion)\n",
    "- `float16` autocast (mixed precision via tensor cores)\n",
    "- `GradScaler` (prevents float16 gradient underflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_loader, val_loader, train_config, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Quick Generation Test\n",
    "\n",
    "Generate text from the just-trained model to verify it learned something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate import generate\n",
    "\n",
    "prompts = [\n",
    "    \"The meaning of life is\",\n",
    "    \"In a shocking finding, scientists discovered\",\n",
    "    \"The history of the United States\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"-\" * 60)\n",
    "    output = generate(\n",
    "        model, prompt=prompt,\n",
    "        max_new_tokens=100, temperature=0.8,\n",
    "        top_k=50, top_p=0.95, device=device,\n",
    "    )\n",
    "    print(output)\n",
    "    print(\"=\" * 60)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}